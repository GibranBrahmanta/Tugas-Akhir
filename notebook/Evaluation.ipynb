{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6c757e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import used modules\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from rouge import Rouge\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1180ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure filepath\n",
    "\n",
    "os.chdir(\"../model_result/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d26c47",
   "metadata": {},
   "source": [
    "# Create Evaluation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dd75ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init ROUGE-N model\n",
    "\n",
    "lst_metrics = ['rouge-1', 'rouge-2', 'rouge-3']\n",
    "rouge = Rouge(metrics=lst_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd132ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_token(document):\n",
    "    \"\"\"Filter all token in document that has non alphabet character and\n",
    "    lowercasing all token that has pass the filter\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    document: two dimensional list\n",
    "        document that want to be filtered\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of token that has pass the filter\n",
    "    \"\"\"\n",
    "    \n",
    "    lst_cleaned = []\n",
    "    for lst_token in document:\n",
    "        for token in lst_token:\n",
    "            if re.match('[a-zA-Z]+', token):\n",
    "                lst_cleaned.append(token.lower())             \n",
    "    return lst_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58454287",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(lst_token):\n",
    "    \"\"\"Convert list of string into string\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    lst_token: list\n",
    "        list of string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        sentence formatted string \n",
    "    \"\"\"\n",
    "    \n",
    "    return \" \".join(lst_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874e7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(hypothesis, reference):\n",
    "    \"\"\"Get ROUGE-N score of a hypothesis based on a reference\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hypothesis: two dimensional list\n",
    "        summary generated by the model\n",
    "    reference: two dimensional list\n",
    "        gold standard summary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        ROUGE-N score in dictionary format\n",
    "    \"\"\"\n",
    "    \n",
    "    cleaned_hypo = clean_token(hypothesis)\n",
    "    cleaned_ref = clean_token(reference)\n",
    "    converted_hypo = convert(cleaned_hypo)\n",
    "    converted_ref = convert(cleaned_ref)\n",
    "    return rouge.get_scores(converted_hypo, converted_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d8aef",
   "metadata": {},
   "source": [
    "# Run Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc020b7",
   "metadata": {},
   "source": [
    "## One File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8eb0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE-N score for all file\n",
    "\n",
    "filename = \"\" # Fill with filepath of a file that want to be checked\n",
    "data = json.load(open(filename))\n",
    "\n",
    "tmp = []\n",
    "\n",
    "for key in data.keys():\n",
    "    tmp2 = [key]\n",
    "    tmp_dct = data[key]\n",
    "    try:\n",
    "        res = score(tmp_dct['hypotesis'], tmp_dct['reference'])\n",
    "    except:\n",
    "        continue\n",
    "    for metric in lst_metrics:\n",
    "        tmp2.append(res[0][metric]['r'])\n",
    "    tmp.append(tmp2)\n",
    "\n",
    "df = pd.DataFrame(tmp, columns=[\"id\"] + lst_metrics)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc75525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print how many data that can't be checked\n",
    "\n",
    "nrow = len(list(data.keys())) - df.shape[0]\n",
    "\n",
    "print(\"Number of mistaken data: {}\".format(nrow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5250a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the final result\n",
    "\n",
    "fin_res = df.loc[:,lst_metrics].mean(axis=0)\n",
    "fin_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c011ec",
   "metadata": {},
   "source": [
    "## Multiple File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e05184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill all list with used dataset/method on all file that want to be checked\n",
    "\n",
    "lst_dataset = [\"IndoSUM\", \"Liputan6\"]\n",
    "lst_topic_modelling = [\"LDA\", \"LSA\", \"NMF\"]\n",
    "lst_embedding = [\"Word2Vec\", \"FastText\", \"TF-IDF\", \"BoW\", \"BERT\"]\n",
    "lst_similarity = [\"Cosine\", \"Euclidean\", \"Jaccard\"]\n",
    "lst_method = [\"Individual\", \"Combined\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c620b0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the ROUGE-N score\n",
    "\n",
    "counter = len(lst_dataset) * len(lst_topic_modelling) * len(lst_embedding) * len(lst_similarity) * len(lst_method)\n",
    "pbar = tqdm(total=counter)\n",
    "\n",
    "lst_res = []\n",
    "\n",
    "for dataset in lst_dataset:\n",
    "    for topic in lst_topic_modelling:\n",
    "        for embedding in lst_embedding:\n",
    "            for similarity in lst_similarity:\n",
    "                for method in lst_method:\n",
    "                    filename = \"{}-{}-{}-{}-{}.json\".format(dataset, topic, embedding, similarity, method)\n",
    "                    data = json.load(open(filename))\n",
    "                    tmp = []\n",
    "                    for key in data.keys():\n",
    "                        tmp2 = [key]\n",
    "                        tmp_dct = data[key]\n",
    "                        try:\n",
    "                            res = score(tmp_dct['hypotesis'], tmp_dct['reference'])\n",
    "                        except:\n",
    "                            continue\n",
    "                        for metric in lst_metrics:\n",
    "                            tmp2.append(res[0][metric]['r'])\n",
    "                        tmp.append(tmp2)\n",
    "                    df = pd.DataFrame(tmp, columns=[\"id\"] + lst_metrics)\n",
    "                    fin_res = df.loc[:,lst_metrics].mean(axis=0)\n",
    "                    tmp = [filename]\n",
    "                    for metrics in lst_metrics:\n",
    "                        tmp.append(fin_res[metrics])\n",
    "                    lst_res.append(tmp)\n",
    "                    pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d929c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the result\n",
    "\n",
    "res_df = pd.DataFrame(lst_res, columns=[\"Filename\"]+lst_metrics)\n",
    "res_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
